{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a10da51",
   "metadata": {},
   "source": [
    "# File Description: Runyankore Word Frequency Analysis\n",
    "\n",
    "This notebook is used to build structured lists of words appearing in the `question_content` column of `questions_nyn.csv`.  \n",
    "The process is designed to first identify the most frequently occurring vocabulary, then filter out non-informative tokens (stop-words), and finally extract more informative words suitable for constructing a classification dictionary.\n",
    "\n",
    "In the first stage, a list of the 3000 most frequent words is generated.  \n",
    "This list is reviewed outside the notebook (in ChatGPT), where the file `nyn_non_category_words.csv` is created to store words not associated with any thematic category.  \n",
    "In the second stage, these stop-words are applied as a filter during frequency recomputation, enabling the extraction of an additional list of approximately 5000 more informative words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c68d9",
   "metadata": {},
   "source": [
    "### Stage 1 – Generate initial frequency list (top 3000 words)\n",
    "\n",
    "In the first stage:\n",
    "\n",
    "1. The dataset is loaded from `questions_nyn.csv`.  \n",
    "2. The text is cleaned (lowercased, special characters removed, spacing normalised).  \n",
    "3. Each question is tokenised, and a set of unique tokens per question is used (each word counted at most once per question).  \n",
    "4. For each word, the number of questions in which it appears is counted.  \n",
    "5. The 3000 most frequent words are selected and saved to `nyn_top_words.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddce6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3000 most frequent words to 'nyn_top_words.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def generate_top_nyn(n_top=3000):\n",
    "    # Load entire file\n",
    "    df = pd.read_csv(\"questions_nyn.csv\")\n",
    "\n",
    "    def clean(text):\n",
    "        text = str(text).lower()\n",
    "        # only letters and spaces - no dots or other characters\n",
    "        text = re.sub(r\"[^a-zA-Z\\u00C0-\\u024F\\u1E00-\\u1EFF\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    df[\"clean\"] = df[\"question_content\"].apply(clean)\n",
    "\n",
    "    words = Counter()\n",
    "\n",
    "    # we count words max once per question\n",
    "    for row in df[\"clean\"]:\n",
    "        tokens = set(row.split())           # <- set = no repetition in one question\n",
    "        for w in tokens:\n",
    "            if len(w) > 2:                  # we reject one-word garbage\n",
    "                words[w] += 1               # how many QUESTIONS did this word contain?\n",
    "\n",
    "    # TOP n_top words\n",
    "    top_words = words.most_common(n_top)\n",
    "\n",
    "    with open(\"nyn_top_words.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"word\", \"question_count\"])\n",
    "        writer.writerows(top_words)\n",
    "\n",
    "    return f\"Saved {len(top_words)} most frequent words to 'nyn_top_words.csv'.\"\n",
    "\n",
    "msg = generate_top_nyn(3000)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8acaa",
   "metadata": {},
   "source": [
    "### Intermediate step – Stop-word selection (ChatGPT)\n",
    "\n",
    "After the file `nyn_top_words.csv` is generated, an intermediate step is performed outside the notebook:\n",
    "\n",
    "1. The list of the 3000 most frequent words is analysed in ChatGPT.  \n",
    "2. Words lacking thematic relevance (pronouns, particles, fillers, purely grammatical forms, etc.) are identified.  \n",
    "3. These non-informative tokens are saved in `nyn_non_category_words.csv`, which serves as the stop-word list.\n",
    "\n",
    "The purpose of this step is to reduce high-frequency but low-information words before generating the next vocabulary list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b0adf",
   "metadata": {},
   "source": [
    "### Stage 2 – Apply stop-words and extract next 5000 informative words\n",
    "\n",
    "In the second stage:\n",
    "\n",
    "1. The file `questions_nyn.csv` is loaded again and cleaned using the same function as in Stage 1.  \n",
    "2. The stop-word list in `nyn_non_category_words.csv` is applied.  \n",
    "3. All stop-words are removed from the question texts.  \n",
    "4. For the remaining tokens, the number of questions containing each word is recalculated (each word counted at most once per question).  \n",
    "5. Approximately 5000 of the most frequent filtered words are saved to `nyn_next5000_words.csv`.\n",
    "\n",
    "The resulting list contains vocabulary with higher informational value, suitable for constructing a classification dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Saved 5000 additional words to: nyn_next5000_words.csv\n",
      "Total unique words considered (after filtering): 326910\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_next_5000_words():\n",
    "    # ----- 1. Load full questions -----\n",
    "    df = pd.read_csv(\"questions_nyn.csv\")   # adjust path if needed\n",
    "\n",
    "    # Cleaning function – same logic as before\n",
    "    def clean(text):\n",
    "        text = str(text).lower()\n",
    "        # keep only letters and spaces\n",
    "        text = re.sub(r\"[^a-zA-Z\\u00C0-\\u024F\\u1E00-\\u1EFF\\s]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    df[\"clean\"] = df[\"question_content\"].apply(clean)\n",
    "\n",
    "    # ----- 2. Load words to ignore (from nyn_non_category_words.csv) -----\n",
    "    ignore_df = pd.read_csv(\"nyn_non_category_words.csv\", header=None)\n",
    "    ignore_words = (\n",
    "        ignore_df.iloc[0]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.lower()\n",
    "        .tolist()\n",
    "    )\n",
    "    ignore_words = set(ignore_words)\n",
    "\n",
    "    # ----- 3. Count word frequencies on full corpus (one count per question) -----\n",
    "    freq = Counter()\n",
    "\n",
    "    for row in df[\"clean\"]:\n",
    "        tokens = set(row.split())  # count each word at most once per question\n",
    "        for w in tokens:\n",
    "            if len(w) <= 2:\n",
    "                continue\n",
    "            if w in ignore_words:\n",
    "                continue\n",
    "            freq[w] += 1\n",
    "\n",
    "    # ----- 4. Build DataFrame, sort and take next 5000 words -----\n",
    "    freq_df = pd.DataFrame(\n",
    "        [(w, c) for w, c in freq.items()],\n",
    "        columns=[\"word\", \"question_count\"]\n",
    "    ).sort_values(\"question_count\", ascending=False)\n",
    "\n",
    "    next_5000 = freq_df.head(5000)\n",
    "\n",
    "    output_file = \"nyn_next5000_words.csv\"\n",
    "    next_5000.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"✅ Done. Saved 5000 additional words to:\", output_file)\n",
    "    print(\"Total unique words considered (after filtering):\", len(freq_df))\n",
    "\n",
    "extract_next_5000_words()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
